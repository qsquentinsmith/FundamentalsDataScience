{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.4"},"colab":{"name":"Feature_Selection.ipynb","provenance":[],"collapsed_sections":["pPL4XFXPeZqR","UOlrgitneZqS","WPleV6MgeZqU","dwbxvh2ieZqY","1bdQnMt1eZqa"]}},"cells":[{"cell_type":"markdown","metadata":{"id":"qzZ0NSlMeZqD"},"source":["# Lab 6\n","\n","## Author: Quentin Smith\n","\n","## Date: 2/29/20"]},{"cell_type":"markdown","metadata":{"id":"gJHWQIEpeZqJ"},"source":["# 1. Overview"]},{"cell_type":"markdown","metadata":{"id":"Y5V5lIlOeZqJ"},"source":["### a.) Objective: \n","We will be attempting to analyze the data described later by two models: logistic regression and random forests. We will run the logistic regression model once with all the data and the following with reduced dimensionality of the data using stepwise selection. We will then try using the Random Forest model twice, reducing the dimensionality once by Principle Compnent Analysis (PCA) and then by feature importance feature selection. "]},{"cell_type":"markdown","metadata":{"id":"VW7qf1PYeZqJ"},"source":["### b.) Background Information:"]},{"cell_type":"markdown","metadata":{"id":"EGZMpGhCeZqK"},"source":["**Logistic Regression** is the go-to method for binary classification problems (problems with two class values) [1]. Logistic Regression models the probability of the default class based on the variables. But that brings up the question, how do we choose the variables to use in Logistic Regression? One way is to use **stepwise selection**. \n","\n","There are two types of stepwise procedures: backward elimination and forward selection. **Backward selection** starts with all variables and statistically eliminates them until only relevant variables remain. The basic steps are as follows [2]:\n","\n","1. Start with all the predictors in the model\n","2. Remove the predictor with the highest p-value greater than $\\alpha_c$\n","3. Refit the moddel and redo step 2\n","4. Stop when all p-values are less than $\\alpha_c$\n","\n","**Forward selection** is just the reverse of the backwards model. The steps are as follows [2]:\n","\n","1. Start with no variables in the model\n","2. For all predictors not in the model, check their p-value if they are added to the model. Choose the one with the lowest p-value less than $\\alpha_c$\n","3. Continue until no new predictors can be added\n","\n","It can be difficult to determine the important charactersitics of a data set. Why have a data set with more dimensionality if the same information is shown with less variables? \n","\n","The **Random Forest** model creates trees that have no correlation or weak correlation, by using bagging and feature randomness; with each decision the model gives one prediction (in the case of our data, who would be interested in buying caravan insurance policies or not). Random Forests have various advantages some of which include: running efficiently with large data bases, gives estimates of what variables are important in the classification, can handle thousands of variables without variable deletion [4]. \n","\n","The first feature selection for Random Forest will be carried out by **Principle Component Analysis (PCA)**. PCA is a statistical solution to dimensional reduction in data sets. It can be thought of as a projection method where data with m-columns (features) is projected into a subspace with m or fewer columns, whilst retaining the essence of the original data [5].\n","\n","The second feature selection for Random Forest will be **Feature Independence**. Feature importance gives you a score for each feature of your data, the higher the score more important or relevant is the feature towards your output variable [6]. It is included in the random forest library.\n","\n","Resources:\n","\n","1. https://machinelearningmastery.com/logistic-regression-for-machine-learning/\n","2. http://www.biostat.jhsph.edu/~iruczins/teaching/jf/ch10.pdf\n","3. https://www.kaggle.com/uciml/caravan-insurance-challenge/home\n","4. https://www.stat.berkeley.edu/~breiman/RandomForests/cc_home.htm#overview\n","5. https://machinelearningmastery.com/calculate-principal-component-analysis-scratch-python/"]},{"cell_type":"markdown","metadata":{"id":"_XzBj1cFeZqK"},"source":["# 2. Data:"]},{"cell_type":"markdown","metadata":{"id":"uAZ9KEJGeZqK"},"source":["The data we will be working with can be downloaded from the Caravan Insurance Challenge dataset at https://www.kaggle.com/uciml/caravan-insurance-challenge/home\n","\n","This data set used in the CoIL 2000 Challenge contains information on customers of an insurance company. The data consists of 86 variables and includes product usage data and socio-demographic data derived from zip area codes. The data was collected to answer the following question: Can you predict who would be interested in buying a caravan insurance policy and give an explanation why[3]?"]},{"cell_type":"code","metadata":{"id":"8nlG8nJreZqL"},"source":["import pandas as pd\n","from IPython.display import display  #Output multiple things in a cell\n","import matplotlib.pyplot as plt\n","import numpy as np\n","from sklearn import metrics\n","from sklearn.metrics import mean_squared_error, r2_score\n","import seaborn as sns; sns.set()\n","from sklearn.linear_model import LinearRegression\n","from sklearn.decomposition import PCA"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"GpEN2-ImeZqM"},"source":["df = pd.read_csv(\"/Users/hv2486co/Desktop/inProgress/inet4061/caravan-insurance-challenge.csv\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3qqolNHXeZqM"},"source":["# 2. Exploratory Data Analysis"]},{"cell_type":"markdown","metadata":{"id":"8QCP-lygeZqN"},"source":["Let us first start by looking at our data so we can approach the analysis with some information on the data."]},{"cell_type":"code","metadata":{"id":"aZkfpn0MeZqN","outputId":"b5189e03-f4d2-42b3-a920-340b5b3012c7"},"source":["df.shape"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(9822, 87)"]},"metadata":{"tags":[]},"execution_count":153}]},{"cell_type":"code","metadata":{"id":"UgtpDWGDeZqO","outputId":"915750d2-1828-4815-cdf6-f40282a896df"},"source":["df.isnull()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>ORIGIN</th>\n","      <th>MOSTYPE</th>\n","      <th>MAANTHUI</th>\n","      <th>MGEMOMV</th>\n","      <th>MGEMLEEF</th>\n","      <th>MOSHOOFD</th>\n","      <th>MGODRK</th>\n","      <th>MGODPR</th>\n","      <th>MGODOV</th>\n","      <th>MGODGE</th>\n","      <th>...</th>\n","      <th>APERSONG</th>\n","      <th>AGEZONG</th>\n","      <th>AWAOREG</th>\n","      <th>ABRAND</th>\n","      <th>AZEILPL</th>\n","      <th>APLEZIER</th>\n","      <th>AFIETS</th>\n","      <th>AINBOED</th>\n","      <th>ABYSTAND</th>\n","      <th>CARAVAN</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>0</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>...</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","    </tr>\n","    <tr>\n","      <td>1</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>...</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>...</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","    </tr>\n","    <tr>\n","      <td>3</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>...</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","    </tr>\n","    <tr>\n","      <td>4</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>...</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","    </tr>\n","    <tr>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <td>9817</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>...</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","    </tr>\n","    <tr>\n","      <td>9818</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>...</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","    </tr>\n","    <tr>\n","      <td>9819</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>...</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","    </tr>\n","    <tr>\n","      <td>9820</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>...</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","    </tr>\n","    <tr>\n","      <td>9821</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>...</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>9822 rows × 87 columns</p>\n","</div>"],"text/plain":["      ORIGIN  MOSTYPE  MAANTHUI  MGEMOMV  MGEMLEEF  MOSHOOFD  MGODRK  MGODPR  \\\n","0      False    False     False    False     False     False   False   False   \n","1      False    False     False    False     False     False   False   False   \n","2      False    False     False    False     False     False   False   False   \n","3      False    False     False    False     False     False   False   False   \n","4      False    False     False    False     False     False   False   False   \n","...      ...      ...       ...      ...       ...       ...     ...     ...   \n","9817   False    False     False    False     False     False   False   False   \n","9818   False    False     False    False     False     False   False   False   \n","9819   False    False     False    False     False     False   False   False   \n","9820   False    False     False    False     False     False   False   False   \n","9821   False    False     False    False     False     False   False   False   \n","\n","      MGODOV  MGODGE  ...  APERSONG  AGEZONG  AWAOREG  ABRAND  AZEILPL  \\\n","0      False   False  ...     False    False    False   False    False   \n","1      False   False  ...     False    False    False   False    False   \n","2      False   False  ...     False    False    False   False    False   \n","3      False   False  ...     False    False    False   False    False   \n","4      False   False  ...     False    False    False   False    False   \n","...      ...     ...  ...       ...      ...      ...     ...      ...   \n","9817   False   False  ...     False    False    False   False    False   \n","9818   False   False  ...     False    False    False   False    False   \n","9819   False   False  ...     False    False    False   False    False   \n","9820   False   False  ...     False    False    False   False    False   \n","9821   False   False  ...     False    False    False   False    False   \n","\n","      APLEZIER  AFIETS  AINBOED  ABYSTAND  CARAVAN  \n","0        False   False    False     False    False  \n","1        False   False    False     False    False  \n","2        False   False    False     False    False  \n","3        False   False    False     False    False  \n","4        False   False    False     False    False  \n","...        ...     ...      ...       ...      ...  \n","9817     False   False    False     False    False  \n","9818     False   False    False     False    False  \n","9819     False   False    False     False    False  \n","9820     False   False    False     False    False  \n","9821     False   False    False     False    False  \n","\n","[9822 rows x 87 columns]"]},"metadata":{"tags":[]},"execution_count":154}]},{"cell_type":"code","metadata":{"id":"RNRa5NwdeZqO","outputId":"966ab5e0-1a93-45aa-f482-e68acca15bba"},"source":["df.describe()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>MOSTYPE</th>\n","      <th>MAANTHUI</th>\n","      <th>MGEMOMV</th>\n","      <th>MGEMLEEF</th>\n","      <th>MOSHOOFD</th>\n","      <th>MGODRK</th>\n","      <th>MGODPR</th>\n","      <th>MGODOV</th>\n","      <th>MGODGE</th>\n","      <th>MRELGE</th>\n","      <th>...</th>\n","      <th>APERSONG</th>\n","      <th>AGEZONG</th>\n","      <th>AWAOREG</th>\n","      <th>ABRAND</th>\n","      <th>AZEILPL</th>\n","      <th>APLEZIER</th>\n","      <th>AFIETS</th>\n","      <th>AINBOED</th>\n","      <th>ABYSTAND</th>\n","      <th>CARAVAN</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>count</td>\n","      <td>9822.000000</td>\n","      <td>9822.000000</td>\n","      <td>9822.000000</td>\n","      <td>9822.000000</td>\n","      <td>9822.000000</td>\n","      <td>9822.000000</td>\n","      <td>9822.000000</td>\n","      <td>9822.000000</td>\n","      <td>9822.000000</td>\n","      <td>9822.000000</td>\n","      <td>...</td>\n","      <td>9822.000000</td>\n","      <td>9822.000000</td>\n","      <td>9822.000000</td>\n","      <td>9822.000000</td>\n","      <td>9822.000000</td>\n","      <td>9822.000000</td>\n","      <td>9822.00000</td>\n","      <td>9822.000000</td>\n","      <td>9822.000000</td>\n","      <td>9822.000000</td>\n","    </tr>\n","    <tr>\n","      <td>mean</td>\n","      <td>24.253207</td>\n","      <td>1.108735</td>\n","      <td>2.677561</td>\n","      <td>2.996437</td>\n","      <td>5.779067</td>\n","      <td>0.700672</td>\n","      <td>4.637650</td>\n","      <td>1.050092</td>\n","      <td>3.262981</td>\n","      <td>6.188964</td>\n","      <td>...</td>\n","      <td>0.004582</td>\n","      <td>0.007941</td>\n","      <td>0.004276</td>\n","      <td>0.574018</td>\n","      <td>0.000916</td>\n","      <td>0.005091</td>\n","      <td>0.03146</td>\n","      <td>0.008450</td>\n","      <td>0.013846</td>\n","      <td>0.059662</td>\n","    </tr>\n","    <tr>\n","      <td>std</td>\n","      <td>12.918058</td>\n","      <td>0.412101</td>\n","      <td>0.780701</td>\n","      <td>0.804660</td>\n","      <td>2.874148</td>\n","      <td>1.015107</td>\n","      <td>1.721212</td>\n","      <td>1.011156</td>\n","      <td>1.606287</td>\n","      <td>1.896070</td>\n","      <td>...</td>\n","      <td>0.067535</td>\n","      <td>0.088764</td>\n","      <td>0.071224</td>\n","      <td>0.561255</td>\n","      <td>0.030258</td>\n","      <td>0.077996</td>\n","      <td>0.20907</td>\n","      <td>0.092647</td>\n","      <td>0.117728</td>\n","      <td>0.236872</td>\n","    </tr>\n","    <tr>\n","      <td>min</td>\n","      <td>1.000000</td>\n","      <td>1.000000</td>\n","      <td>1.000000</td>\n","      <td>1.000000</td>\n","      <td>1.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>...</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.00000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>25%</td>\n","      <td>10.000000</td>\n","      <td>1.000000</td>\n","      <td>2.000000</td>\n","      <td>2.000000</td>\n","      <td>3.000000</td>\n","      <td>0.000000</td>\n","      <td>4.000000</td>\n","      <td>0.000000</td>\n","      <td>2.000000</td>\n","      <td>5.000000</td>\n","      <td>...</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.00000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>50%</td>\n","      <td>30.000000</td>\n","      <td>1.000000</td>\n","      <td>3.000000</td>\n","      <td>3.000000</td>\n","      <td>7.000000</td>\n","      <td>0.000000</td>\n","      <td>5.000000</td>\n","      <td>1.000000</td>\n","      <td>3.000000</td>\n","      <td>6.000000</td>\n","      <td>...</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>1.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.00000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>75%</td>\n","      <td>35.000000</td>\n","      <td>1.000000</td>\n","      <td>3.000000</td>\n","      <td>3.000000</td>\n","      <td>8.000000</td>\n","      <td>1.000000</td>\n","      <td>6.000000</td>\n","      <td>2.000000</td>\n","      <td>4.000000</td>\n","      <td>7.000000</td>\n","      <td>...</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>1.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.00000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>max</td>\n","      <td>41.000000</td>\n","      <td>10.000000</td>\n","      <td>6.000000</td>\n","      <td>6.000000</td>\n","      <td>10.000000</td>\n","      <td>9.000000</td>\n","      <td>9.000000</td>\n","      <td>5.000000</td>\n","      <td>9.000000</td>\n","      <td>9.000000</td>\n","      <td>...</td>\n","      <td>1.000000</td>\n","      <td>1.000000</td>\n","      <td>2.000000</td>\n","      <td>7.000000</td>\n","      <td>1.000000</td>\n","      <td>2.000000</td>\n","      <td>4.00000</td>\n","      <td>2.000000</td>\n","      <td>2.000000</td>\n","      <td>1.000000</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>8 rows × 86 columns</p>\n","</div>"],"text/plain":["           MOSTYPE     MAANTHUI      MGEMOMV     MGEMLEEF     MOSHOOFD  \\\n","count  9822.000000  9822.000000  9822.000000  9822.000000  9822.000000   \n","mean     24.253207     1.108735     2.677561     2.996437     5.779067   \n","std      12.918058     0.412101     0.780701     0.804660     2.874148   \n","min       1.000000     1.000000     1.000000     1.000000     1.000000   \n","25%      10.000000     1.000000     2.000000     2.000000     3.000000   \n","50%      30.000000     1.000000     3.000000     3.000000     7.000000   \n","75%      35.000000     1.000000     3.000000     3.000000     8.000000   \n","max      41.000000    10.000000     6.000000     6.000000    10.000000   \n","\n","            MGODRK       MGODPR       MGODOV       MGODGE       MRELGE  ...  \\\n","count  9822.000000  9822.000000  9822.000000  9822.000000  9822.000000  ...   \n","mean      0.700672     4.637650     1.050092     3.262981     6.188964  ...   \n","std       1.015107     1.721212     1.011156     1.606287     1.896070  ...   \n","min       0.000000     0.000000     0.000000     0.000000     0.000000  ...   \n","25%       0.000000     4.000000     0.000000     2.000000     5.000000  ...   \n","50%       0.000000     5.000000     1.000000     3.000000     6.000000  ...   \n","75%       1.000000     6.000000     2.000000     4.000000     7.000000  ...   \n","max       9.000000     9.000000     5.000000     9.000000     9.000000  ...   \n","\n","          APERSONG      AGEZONG      AWAOREG       ABRAND      AZEILPL  \\\n","count  9822.000000  9822.000000  9822.000000  9822.000000  9822.000000   \n","mean      0.004582     0.007941     0.004276     0.574018     0.000916   \n","std       0.067535     0.088764     0.071224     0.561255     0.030258   \n","min       0.000000     0.000000     0.000000     0.000000     0.000000   \n","25%       0.000000     0.000000     0.000000     0.000000     0.000000   \n","50%       0.000000     0.000000     0.000000     1.000000     0.000000   \n","75%       0.000000     0.000000     0.000000     1.000000     0.000000   \n","max       1.000000     1.000000     2.000000     7.000000     1.000000   \n","\n","          APLEZIER      AFIETS      AINBOED     ABYSTAND      CARAVAN  \n","count  9822.000000  9822.00000  9822.000000  9822.000000  9822.000000  \n","mean      0.005091     0.03146     0.008450     0.013846     0.059662  \n","std       0.077996     0.20907     0.092647     0.117728     0.236872  \n","min       0.000000     0.00000     0.000000     0.000000     0.000000  \n","25%       0.000000     0.00000     0.000000     0.000000     0.000000  \n","50%       0.000000     0.00000     0.000000     0.000000     0.000000  \n","75%       0.000000     0.00000     0.000000     0.000000     0.000000  \n","max       2.000000     4.00000     2.000000     2.000000     1.000000  \n","\n","[8 rows x 86 columns]"]},"metadata":{"tags":[]},"execution_count":155}]},{"cell_type":"code","metadata":{"id":"vg7MGsKIeZqP","outputId":"18cb001a-a623-4bef-b041-11bfc21a9f5a"},"source":["df.head(5)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>ORIGIN</th>\n","      <th>MOSTYPE</th>\n","      <th>MAANTHUI</th>\n","      <th>MGEMOMV</th>\n","      <th>MGEMLEEF</th>\n","      <th>MOSHOOFD</th>\n","      <th>MGODRK</th>\n","      <th>MGODPR</th>\n","      <th>MGODOV</th>\n","      <th>MGODGE</th>\n","      <th>...</th>\n","      <th>APERSONG</th>\n","      <th>AGEZONG</th>\n","      <th>AWAOREG</th>\n","      <th>ABRAND</th>\n","      <th>AZEILPL</th>\n","      <th>APLEZIER</th>\n","      <th>AFIETS</th>\n","      <th>AINBOED</th>\n","      <th>ABYSTAND</th>\n","      <th>CARAVAN</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>0</td>\n","      <td>train</td>\n","      <td>33</td>\n","      <td>1</td>\n","      <td>3</td>\n","      <td>2</td>\n","      <td>8</td>\n","      <td>0</td>\n","      <td>5</td>\n","      <td>1</td>\n","      <td>3</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <td>1</td>\n","      <td>train</td>\n","      <td>37</td>\n","      <td>1</td>\n","      <td>2</td>\n","      <td>2</td>\n","      <td>8</td>\n","      <td>1</td>\n","      <td>4</td>\n","      <td>1</td>\n","      <td>4</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>train</td>\n","      <td>37</td>\n","      <td>1</td>\n","      <td>2</td>\n","      <td>2</td>\n","      <td>8</td>\n","      <td>0</td>\n","      <td>4</td>\n","      <td>2</td>\n","      <td>4</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <td>3</td>\n","      <td>train</td>\n","      <td>9</td>\n","      <td>1</td>\n","      <td>3</td>\n","      <td>3</td>\n","      <td>3</td>\n","      <td>2</td>\n","      <td>3</td>\n","      <td>2</td>\n","      <td>4</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <td>4</td>\n","      <td>train</td>\n","      <td>40</td>\n","      <td>1</td>\n","      <td>4</td>\n","      <td>2</td>\n","      <td>10</td>\n","      <td>1</td>\n","      <td>4</td>\n","      <td>1</td>\n","      <td>4</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>5 rows × 87 columns</p>\n","</div>"],"text/plain":["  ORIGIN  MOSTYPE  MAANTHUI  MGEMOMV  MGEMLEEF  MOSHOOFD  MGODRK  MGODPR  \\\n","0  train       33         1        3         2         8       0       5   \n","1  train       37         1        2         2         8       1       4   \n","2  train       37         1        2         2         8       0       4   \n","3  train        9         1        3         3         3       2       3   \n","4  train       40         1        4         2        10       1       4   \n","\n","   MGODOV  MGODGE  ...  APERSONG  AGEZONG  AWAOREG  ABRAND  AZEILPL  APLEZIER  \\\n","0       1       3  ...         0        0        0       1        0         0   \n","1       1       4  ...         0        0        0       1        0         0   \n","2       2       4  ...         0        0        0       1        0         0   \n","3       2       4  ...         0        0        0       1        0         0   \n","4       1       4  ...         0        0        0       1        0         0   \n","\n","   AFIETS  AINBOED  ABYSTAND  CARAVAN  \n","0       0        0         0        0  \n","1       0        0         0        0  \n","2       0        0         0        0  \n","3       0        0         0        0  \n","4       0        0         0        0  \n","\n","[5 rows x 87 columns]"]},"metadata":{"tags":[]},"execution_count":156}]},{"cell_type":"markdown","metadata":{"id":"6lW3JgpSeZqP"},"source":["The data has no null values and has 85 independent vairables. We will remove the ORIGIN column which devides the data into training and test data and we will be seperating the CARAVAN column which is the Target variable of the data set. The CARAVAN column determines whether or not someone is interested in buying a caravan insurance policy or not. "]},{"cell_type":"markdown","metadata":{"id":"3TIcTqC7eZqP"},"source":["The data is split into training and test set based on the ORIGIN column. Let us create two different data sets Train and Test for both the dependent and independent variables."]},{"cell_type":"code","metadata":{"id":"2FGtEbIEeZqP"},"source":["df_train = df[df.ORIGIN == 'train']\n","df_test = df[df.ORIGIN == 'test']\n","\n","df_train_x = df_train.iloc[:, 1:-1]\n","df_train_y = df_train['CARAVAN']\n","\n","df_test_x = df_test.iloc[:, 1:-1]\n","df_test_y = df_test['CARAVAN']"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"bbuebvkmeZqQ","outputId":"e020746e-2403-4007-e4e6-6b684d2d9283"},"source":["df_train_x.shape"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(5822, 85)"]},"metadata":{"tags":[]},"execution_count":158}]},{"cell_type":"code","metadata":{"id":"_a8niOx0eZqQ","outputId":"827a1ad2-54ab-4fb3-f692-69727ca3bd94"},"source":["df_train_y.shape"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(5822,)"]},"metadata":{"tags":[]},"execution_count":159}]},{"cell_type":"code","metadata":{"id":"p7UonajxeZqQ","outputId":"9b8fb944-ba04-4856-b654-5a44d3eaa734"},"source":["df_test_x.shape"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(4000, 85)"]},"metadata":{"tags":[]},"execution_count":160}]},{"cell_type":"code","metadata":{"id":"R-iwVZrPeZqQ","outputId":"08d707f6-1fd1-4243-9b49-7c339cf1a100"},"source":["df_test_y.shape"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(4000,)"]},"metadata":{"tags":[]},"execution_count":161}]},{"cell_type":"markdown","metadata":{"id":"WDmyN1cEeZqR"},"source":["# 3. Models/ Techniques"]},{"cell_type":"markdown","metadata":{"id":"pPL4XFXPeZqR"},"source":["### a.) Logistic Regression with all features (Base)"]},{"cell_type":"code","metadata":{"id":"vQFfo6RJeZqR","outputId":"f9470fea-56b1-4551-e626-80bf62558d37"},"source":["from sklearn.linear_model import LogisticRegression \n","from sklearn.metrics import confusion_matrix , accuracy_score\n","\n","classifier = LogisticRegression(random_state = 0) \n","classifier.fit(df_train_x, df_train_y)\n","\n","y_pred = classifier.predict(df_test_x)\n","\n","\n","cm = confusion_matrix(df_test_y,y_pred) \n","  \n","print (\"Confusion Matrix : \\n\", cm) \n","\n","  \n","print (\"Accuracy : \", accuracy_score(df_test_y,y_pred))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["C:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n","  FutureWarning)\n"],"name":"stderr"},{"output_type":"stream","text":["Confusion Matrix : \n"," [[3757    5]\n"," [ 235    3]]\n","Accuracy :  0.94\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"mkHgyUGkeZqR","outputId":"749d0547-a467-4aa2-9820-d9fe8c810fe3"},"source":["TP = cm[0, 0]\n","TN = cm[1, 1]\n","FP = cm[0, 1]\n","FN = cm[1, 0]\n","\n","print((TP + TN) / float(TP + TN + FP + FN))\n","print(metrics.accuracy_score(df_test_y, y_pred))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["0.9405\n","0.9405\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"5-8CrvHxeZqR","outputId":"db052b1b-d5ea-488d-e5a1-e1fd1192d1f8"},"source":["classification_error = (FP + FN) / float(TP + TN + FP + FN)\n","\n","print(classification_error)\n","print(1 - metrics.accuracy_score(df_test_y, y_pred))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["0.0595\n","0.0595\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"LySrcZs4eZqS","outputId":"a796ed2e-0377-4123-c9c7-340a4fbb7ec8"},"source":["#recall\n","print(TP / float(TP + FN))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["0.9405\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"eJGePcqMeZqS"},"source":["We can see by the Logistic Regression with all features that the accuracy is 0.94 and the recall is 0.9405."]},{"cell_type":"markdown","metadata":{"id":"UOlrgitneZqS"},"source":["### b.) Logistic Regression with Stepise Selection based on p-values"]},{"cell_type":"code","metadata":{"id":"lfozUXeneZqS","outputId":"361f70c4-eacc-4dd2-b859-f0c80c1e0342"},"source":["y = df['CARAVAN']\n","X = df.iloc[:, 1:-1]\n","\n","\n","def stepwise_selection(X, y, \n","                       initial_list=[], \n","                       threshold_in=0.01, \n","                       threshold_out = 0.05, \n","                       verbose=True):\n","\n","    included = list(initial_list)\n","    while True:\n","        changed=False\n","        # forward step\n","        excluded = list(set(X.columns)-set(included))\n","        new_pval = pd.Series(index=excluded)\n","        for new_column in excluded:\n","            model = sm.OLS(y, sm.add_constant(pd.DataFrame(X[included+[new_column]]))).fit()\n","            new_pval[new_column] = model.pvalues[new_column]\n","        best_pval = new_pval.min()\n","        if best_pval < threshold_in:\n","#            best_feature = new_pval.argmin()\n","            best_feature = new_pval.idxmin()\n","            included.append(best_feature)\n","            changed=True\n","            if verbose:\n","                print('Add  {:30} with p-value {:.6}'.format(best_feature, best_pval))\n","\n","        # backward step\n","        model = sm.OLS(y, sm.add_constant(pd.DataFrame(X[included]))).fit()\n","        # use all coefs except intercept\n","        pvalues = model.pvalues.iloc[1:]\n","        worst_pval = pvalues.max() # null if pvalues is empty\n","        if worst_pval > threshold_out:\n","            changed=True\n","            worst_feature = pvalues.argmax()\n","            included.remove(worst_feature)\n","            if verbose:\n","                print('Drop {:30} with p-value {:.6}'.format(worst_feature, worst_pval))\n","        if not changed:\n","            break\n","    return included\n","\n","result = stepwise_selection(X, y)\n","\n","print('resulting features:')\n","print(result)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Add  PPERSAUT                       with p-value 2.14684e-42\n","Add  MKOOPKLA                       with p-value 1.36739e-21\n","Add  PWAPART                        with p-value 3.66711e-15\n","Add  APLEZIER                       with p-value 8.20766e-15\n","Add  MOPLHOOG                       with p-value 4.25236e-06\n","Add  PBRAND                         with p-value 3.92829e-06\n","Add  MBERBOER                       with p-value 8.31838e-06\n","Add  MRELGE                         with p-value 1.41977e-05\n","Add  PWALAND                        with p-value 0.000361295\n","Add  ABRAND                         with p-value 0.000937601\n","Add  AZEILPL                        with p-value 0.00153041\n","Add  MINK123M                       with p-value 0.00152554\n","Add  PBYSTAND                       with p-value 0.00243579\n","Add  PGEZONG                        with p-value 0.00485648\n","Add  AGEZONG                        with p-value 0.00450709\n","Add  MHHUUR                         with p-value 0.00630075\n","resulting features:\n","['PPERSAUT', 'MKOOPKLA', 'PWAPART', 'APLEZIER', 'MOPLHOOG', 'PBRAND', 'MBERBOER', 'MRELGE', 'PWALAND', 'ABRAND', 'AZEILPL', 'MINK123M', 'PBYSTAND', 'PGEZONG', 'AGEZONG', 'MHHUUR']\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"WpZJgdNaeZqT","outputId":"1fa6b075-735b-4ffa-ef7f-c322beb6698f"},"source":["\n","X_fselect = df_train_x[['PPERSAUT', 'MKOOPKLA', 'PWAPART', 'APLEZIER', 'MOPLHOOG', 'PBRAND', 'MBERBOER', 'MRELGE', \n","                'PWALAND', 'ABRAND', 'AZEILPL', 'MINK123M', 'PBYSTAND', 'PGEZONG', 'AGEZONG', 'MHHUUR']]\n","X_testfselect = df_test_x[['PPERSAUT', 'MKOOPKLA', 'PWAPART', 'APLEZIER', 'MOPLHOOG', 'PBRAND', 'MBERBOER', 'MRELGE', \n","                'PWALAND', 'ABRAND', 'AZEILPL', 'MINK123M', 'PBYSTAND', 'PGEZONG', 'AGEZONG', 'MHHUUR']]\n","Y = df_train_y\n"," \n","classifier = LogisticRegression(random_state = 0) \n","classifier.fit(X_fselect, Y)\n","\n","y_pred = classifier.predict(X_testfselect)\n","\n","\n","\n","cm = confusion_matrix(df_test_y,y_pred) \n","  \n","print (\"Confusion Matrix : \\n\", cm) \n","\n","  \n","print (\"Accuracy : \", accuracy_score(df_test_y,y_pred))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Confusion Matrix : \n"," [[3759    3]\n"," [ 235    3]]\n","Accuracy :  0.9405\n"],"name":"stdout"},{"output_type":"stream","text":["C:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n","  FutureWarning)\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"SgEXUUggeZqU","outputId":"b2e33e8b-60b6-405b-fd4c-3b4ccd3ef521"},"source":["TP = cm[0, 0]\n","TN = cm[1, 1]\n","FP = cm[0, 1]\n","FN = cm[1, 0]\n","\n","#recall\n","print(TP / float(TP + FN))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["0.9405\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"qDHeBvD4eZqU"},"source":["We can see by the Logistic Regression with features chosen by stepwise selection has the same accuracy at 0.94. 16 features were chosen. We also see two less True Negatives and False Positives were present in this confusion matrix."]},{"cell_type":"markdown","metadata":{"id":"WPleV6MgeZqU"},"source":["### c.) Random Forest based on Principle component analysis feature selction"]},{"cell_type":"code","metadata":{"id":"ky_FempTeZqV","outputId":"5f457c4a-4ba8-44c5-a0d5-e04f34ed6f88"},"source":["all_vars = df.drop(['ORIGIN'], axis=1)\n","x_vars = df_train_x\n","y = df_train_y\n","\n","# principal component analysis \n","\n","# standardize the data \n","X_std = StandardScaler().fit_transform(x_vars)\n","mean_vec = np.mean(X_std, axis=0)\n","cov_mat = (X_std - mean_vec).T.dot((X_std - mean_vec)) / (X_std.shape[0]-1)\n","print('Covariance matrix \\n%s' %cov_mat)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Covariance matrix \n","[[ 1.00017179 -0.03872791 -0.02200074 ... -0.01577627 -0.02109104\n","  -0.0537268 ]\n"," [-0.03872791  1.00017179  0.01010398 ... -0.02099688  0.01830694\n","  -0.00416694]\n"," [-0.02200074  0.01010398  1.00017179 ...  0.03033527  0.02591114\n","   0.02838864]\n"," ...\n"," [-0.01577627 -0.02099688  0.03033527 ...  1.00017179  0.00484596\n","   0.00924747]\n"," [-0.02109104  0.01830694  0.02591114 ...  0.00484596  1.00017179\n","   0.02127682]\n"," [-0.0537268  -0.00416694  0.02838864 ...  0.00924747  0.02127682\n","   1.00017179]]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"rh35p_0ZeZqV","outputId":"53ac65f1-2593-4009-d4e8-3ef5d616df2b"},"source":["#Perform eigendecomposition on covariance matrix\n","cov_mat = np.cov(X_std.T)\n","eig_vals, eig_vecs = np.linalg.eig(cov_mat)\n","print('Eigenvectors \\n%s' %eig_vecs)\n","print('\\nEigenvalues \\n%s' %eig_vals)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Eigenvectors \n","[[-1.80741073e-01  1.90012238e-01 -1.08200120e-02 ... -1.19823729e-02\n","   1.86481597e-03 -3.99575753e-04]\n"," [ 6.86446049e-05 -9.97766755e-03  1.42353206e-02 ... -5.62264287e-03\n","   1.64017734e-03 -1.88420999e-03]\n"," [ 1.21471618e-01  2.78081107e-01  9.89312897e-02 ...  1.32823262e-03\n","  -4.33591381e-03 -1.66503718e-03]\n"," ...\n"," [ 1.26127757e-02 -1.12937278e-02  2.14048183e-02 ...  9.27489842e-03\n","  -2.15410748e-02 -4.32546502e-03]\n"," [ 1.32624344e-02 -5.62562226e-03 -1.97657957e-02 ...  1.93069813e-03\n","  -1.94621753e-03 -5.83645630e-03]\n"," [ 2.36962778e-02  1.36883444e-02 -4.79283762e-02 ...  4.76677274e-02\n","   1.50691498e-02 -3.00046503e-02]]\n","\n","Eigenvalues \n","[9.38114298e+00 4.90789884e+00 3.98336652e+00 3.34322942e+00\n"," 2.92824840e+00 2.64384083e+00 2.26219960e+00 2.23659410e+00\n"," 2.14010672e+00 2.10857567e+00 2.00996208e+00 1.95746407e+00\n"," 1.91389055e+00 1.90310544e+00 1.86888982e+00 1.84630241e+00\n"," 1.79586996e+00 1.81596659e+00 1.72663381e+00 1.68813177e+00\n"," 1.55440514e+00 1.64806302e+00 1.62031830e+00 1.38457956e+00\n"," 1.45373536e+00 1.42746322e+00 1.31847563e+00 1.27461790e+00\n"," 1.13484625e+00 1.19260062e+00 1.20852203e+00 1.05794702e+00\n"," 9.73410107e-01 9.48684989e-01 8.76109506e-01 8.52179201e-01\n"," 7.13488960e-01 7.73204970e-01 7.95741040e-01 8.14269572e-01\n"," 7.53824663e-01 6.19100476e-01 6.79104244e-01 5.62045509e-01\n"," 5.95793872e-01 4.63305819e-01 4.69505046e-01 3.81630781e-01\n"," 3.21680502e-01 3.38308150e-01 1.97275996e-01 1.70937409e-01\n"," 1.46517388e-01 1.44544847e-01 1.34015386e-01 1.26700732e-01\n"," 1.15349363e-01 1.07555511e-01 1.03837949e-01 9.80237167e-02\n"," 5.45284011e-03 2.62882378e-04 6.41300056e-04 9.12843707e-02\n"," 8.99486814e-02 8.16324392e-02 5.44035528e-02 6.23525567e-02\n"," 7.75176397e-02 1.52485450e-02 6.65622373e-02 1.70374630e-02\n"," 4.40246373e-02 2.63267265e-02 1.84601060e-02 1.94393481e-02\n"," 3.45140033e-02 3.94323005e-02 3.13134055e-02 2.86393954e-02\n"," 2.79075675e-02 7.48788307e-02 3.22847283e-02 1.97996292e-02\n"," 3.61504284e-02]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"uXrsrcXseZqV","outputId":"cece6a62-0e8f-4595-e7f1-8930bf8abd47"},"source":["for ev in eig_vecs:\n","    np.testing.assert_array_almost_equal(1.0, np.linalg.norm(ev))\n","print('Everything ok!')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Everything ok!\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"V8ijBbCJeZqW","outputId":"c5d99213-1ba7-4bad-c567-fa678fdb8444"},"source":["pca = PCA().fit(x_vars)\n","plt.plot(np.cumsum(pca.explained_variance_ratio_))\n","plt.xlabel('number of components')\n","plt.ylabel('cumulative explained variance');\n","plt.plot(np.cumsum(pca.explained_variance_ratio_))\n","plt.xlabel('number of components')\n","plt.ylabel('cumulative explained variance');"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"tags":[]}}]},{"cell_type":"code","metadata":{"id":"K79qjPV0eZqW","outputId":"7fd23982-3922-4842-ae58-eb6f81eb5993"},"source":["pca = PCA(0.90).fit(x_vars)\n","feat = pca.components_\n","pca.n_components_"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["13"]},"metadata":{"tags":[]},"execution_count":245}]},{"cell_type":"code","metadata":{"id":"YISFgajmeZqW"},"source":["# y  = pca.explained_variance_ratio_\n","# x = pca.components_\n","\n","# model = RandomForestClassifier(n_estimators = 100)\n","\n","# model.fit(x, y)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"naKBBBUyeZqX"},"source":["We see that 90% of the variance can be described by 13 principle components. Therefore, the first 13 eigenvectors should be used to construct the dimensions of the new fefature space. "]},{"cell_type":"markdown","metadata":{"id":"89-J6M2QeZqX"},"source":["Now to transform test data the same as training data."]},{"cell_type":"code","metadata":{"id":"l0tiwIcseZqX","outputId":"11011038-d2ad-46db-9b93-63ddf56ee2e0"},"source":["all_vars = df.drop(['ORIGIN'], axis=1)\n","x_vars_test = df_test_x\n","y = df_test_y\n","\n","# principal component analysis on test data\n","\n","# standardize the data \n","X_std = StandardScaler().fit_transform(x_vars_test)\n","mean_vec = np.mean(X_std, axis=0)\n","cov_mat = (X_std - mean_vec).T.dot((X_std - mean_vec)) / (X_std.shape[0]-1)\n","print('Covariance matrix \\n%s' %cov_mat)\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Covariance matrix \n","[[ 1.00025006e+00 -4.25692259e-02  1.61737437e-02 ... -3.98864355e-02\n","  -9.90213475e-03 -2.87885575e-02]\n"," [-4.25692259e-02  1.00025006e+00 -2.51737327e-02 ... -4.14674354e-04\n","   5.01174846e-02 -8.40437329e-03]\n"," [ 1.61737437e-02 -2.51737327e-02  1.00025006e+00 ...  3.27122766e-04\n","  -1.02235172e-02  3.47341760e-02]\n"," ...\n"," [-3.98864355e-02 -4.14674354e-04  3.27122766e-04 ...  1.00025006e+00\n","  -1.86174153e-03  3.78539904e-03]\n"," [-9.90213475e-03  5.01174846e-02 -1.02235172e-02 ... -1.86174153e-03\n","   1.00025006e+00  1.16450516e-02]\n"," [-2.87885575e-02 -8.40437329e-03  3.47341760e-02 ...  3.78539904e-03\n","   1.16450516e-02  1.00025006e+00]]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"qpZsxzjveZqX","outputId":"98d3229b-f8aa-444b-b4c6-1a00fe58e3b1"},"source":["#Perform eigendecomposition on covariance matrix\n","cov_mat = np.cov(X_std.T)\n","eig_vals, eig_vecs = np.linalg.eig(cov_mat)\n","print('Eigenvectors \\n%s' %eig_vecs)\n","print('\\nEigenvalues \\n%s' %eig_vals)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Eigenvectors \n","[[-1.81065422e-01 -1.93411358e-01 -6.24242198e-03 ... -1.17910635e-02\n","   5.78934938e-03 -1.48520840e-03]\n"," [-3.70485915e-03  2.29829284e-02 -7.78147821e-03 ...  3.19395600e-04\n","  -3.03366670e-03 -3.80847642e-04]\n"," [ 1.15882686e-01 -2.79225214e-01 -9.87262758e-02 ...  2.78580132e-03\n","  -5.03957057e-03 -2.06825200e-03]\n"," ...\n"," [ 7.10608786e-03  1.98641804e-02 -1.35110680e-02 ...  1.23270806e-03\n","   2.53668451e-03  2.45137149e-03]\n"," [ 1.60178519e-02  1.49008244e-02  3.06569714e-02 ... -2.38501038e-03\n","  -5.98808733e-03 -9.71292014e-04]\n"," [ 1.60948934e-02 -2.62533932e-02  8.54319218e-02 ... -1.53261649e-02\n","  -9.05950550e-05 -3.46357692e-02]]\n","\n","Eigenvalues \n","[9.44087455e+00 4.92163028e+00 4.04744736e+00 3.33473614e+00\n"," 2.87228757e+00 2.56689238e+00 2.34376122e+00 2.31275043e+00\n"," 2.27143150e+00 2.11875801e+00 2.09071064e+00 2.03055440e+00\n"," 1.94533789e+00 1.90324976e+00 1.87988745e+00 1.83518596e+00\n"," 1.79174040e+00 1.85045947e+00 1.71854505e+00 1.69251658e+00\n"," 1.50748106e+00 1.56848820e+00 1.59324743e+00 1.58610236e+00\n"," 1.42228040e+00 1.32815105e+00 1.25694488e+00 1.30847582e+00\n"," 1.19702500e+00 1.14184538e+00 1.11105737e+00 1.06180431e+00\n"," 9.75640322e-01 9.36619550e-01 8.71283813e-01 7.77744702e-01\n"," 8.43801858e-01 8.32727805e-01 7.40720486e-01 7.22857766e-01\n"," 6.83556753e-01 6.12986835e-01 6.37579285e-01 5.76294707e-01\n"," 5.53435913e-01 4.65268966e-01 4.56681017e-01 3.67881500e-01\n"," 3.36019002e-01 3.18152698e-01 2.04082496e-01 1.61970479e-01\n"," 1.45393502e-01 1.40290098e-01 1.37490732e-01 1.33710126e-01\n"," 1.17179037e-01 1.13832843e-01 1.10682218e-01 1.04937425e-01\n"," 9.74494282e-02 9.05907401e-02 8.39281063e-02 2.10766349e-04\n"," 5.36463921e-03 1.48526361e-03 4.34182937e-04 6.11153998e-02\n"," 6.97466312e-02 5.52342263e-02 1.69952111e-02 2.62862240e-02\n"," 4.11718715e-02 4.45962481e-02 3.43450210e-02 3.50012059e-02\n"," 4.32863706e-02 3.04804124e-02 2.02078365e-02 2.86851970e-02\n"," 2.78135680e-02 1.97203238e-02 1.91816764e-02 1.87459571e-02\n"," 1.86965581e-02]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Mis6NrNMeZqX","outputId":"8e586395-874e-497c-82de-75b6d2516993"},"source":["for ev in eig_vecs:\n","    np.testing.assert_array_almost_equal(1.0, np.linalg.norm(ev))\n","print('Everything ok!')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Everything ok!\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"nY1LMovceZqY","outputId":"27cc6afc-9522-4f79-e341-b10e593ba5c6"},"source":["pca = PCA(0.90).fit(x_vars_test)\n","feat_test = pca.components_\n","pca.n_components_"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["13"]},"metadata":{"tags":[]},"execution_count":296}]},{"cell_type":"code","metadata":{"id":"BTkNoHkheZqY"},"source":["# model = RandomForestClassifier(n_estimators = 100)\n","\n","# model.fit("],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"dwbxvh2ieZqY"},"source":["### d.) Random Forest based on feature importance feature selection"]},{"cell_type":"code","metadata":{"id":"MDZo608CeZqY"},"source":["from sklearn.feature_selection import SelectFromModel"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"nrogFNTceZqY","outputId":"9336efc1-5f4e-4ae4-baed-9338bcc110ee"},"source":["sel = SelectFromModel(RandomForestClassifier(n_estimators = 100))\n","sel.fit(df_train_x, df_train_y)\n","sel.get_support()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([ True, False, False, False,  True, False,  True,  True,  True,\n","        True, False,  True,  True,  True,  True,  True,  True,  True,\n","        True, False, False,  True,  True,  True,  True,  True,  True,\n","        True, False,  True,  True,  True, False,  True,  True,  True,\n","        True,  True,  True,  True, False,  True,  True,  True, False,\n","       False,  True, False, False, False, False, False, False, False,\n","       False, False, False, False,  True, False, False, False, False,\n","       False,  True, False, False,  True, False, False, False, False,\n","       False, False, False,  True, False, False, False,  True, False,\n","       False, False, False, False])"]},"metadata":{"tags":[]},"execution_count":213}]},{"cell_type":"code","metadata":{"id":"v-2K7Y9qeZqZ","outputId":"472d1368-40f6-4b76-cacc-7593267608fb"},"source":["selected_feat= df_train_x.columns[(sel.get_support())]\n","len(selected_feat)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["40"]},"metadata":{"tags":[]},"execution_count":215}]},{"cell_type":"code","metadata":{"id":"AC0rDvFseZqZ","outputId":"f661336f-84a3-46a6-a761-d4c7b42e0d68"},"source":["print(selected_feat)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Index(['MOSTYPE', 'MOSHOOFD', 'MGODPR', 'MGODOV', 'MGODGE', 'MRELGE', 'MRELOV',\n","       'MFALLEEN', 'MFGEKIND', 'MFWEKIND', 'MOPLHOOG', 'MOPLMIDD', 'MOPLLAAG',\n","       'MBERHOOG', 'MBERMIDD', 'MBERARBG', 'MBERARBO', 'MSKA', 'MSKB1',\n","       'MSKB2', 'MSKC', 'MHHUUR', 'MHKOOP', 'MAUT1', 'MAUT0', 'MZFONDS',\n","       'MZPART', 'MINKM30', 'MINK3045', 'MINK4575', 'MINK7512', 'MINKGEM',\n","       'MKOOPKLA', 'PWAPART', 'PPERSAUT', 'PBRAND', 'AWAPART', 'APERSAUT',\n","       'ALEVEN', 'ABRAND'],\n","      dtype='object')\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"L8wT2uQ_eZqZ","outputId":"c4ba4ec4-5b9a-4c4a-f0b4-304b156a9b04"},"source":["\n","X_fselect = df_train_x[['MOSTYPE', 'MOSHOOFD', 'MGODPR', 'MGODOV', 'MGODGE', 'MRELGE', 'MRELOV',\n","       'MFALLEEN', 'MFGEKIND', 'MFWEKIND', 'MOPLHOOG', 'MOPLMIDD', 'MOPLLAAG',\n","       'MBERHOOG', 'MBERMIDD', 'MBERARBG', 'MBERARBO', 'MSKA', 'MSKB1',\n","       'MSKB2', 'MSKC', 'MHHUUR', 'MHKOOP', 'MAUT1', 'MAUT0', 'MZFONDS',\n","       'MZPART', 'MINKM30', 'MINK3045', 'MINK4575', 'MINK7512', 'MINKGEM',\n","       'MKOOPKLA', 'PWAPART', 'PPERSAUT', 'PBRAND', 'AWAPART', 'APERSAUT',\n","       'ALEVEN', 'ABRAND']]\n","X_testfselect = df_test_x[['MOSTYPE', 'MOSHOOFD', 'MGODPR', 'MGODOV', 'MGODGE', 'MRELGE', 'MRELOV',\n","       'MFALLEEN', 'MFGEKIND', 'MFWEKIND', 'MOPLHOOG', 'MOPLMIDD', 'MOPLLAAG',\n","       'MBERHOOG', 'MBERMIDD', 'MBERARBG', 'MBERARBO', 'MSKA', 'MSKB1',\n","       'MSKB2', 'MSKC', 'MHHUUR', 'MHKOOP', 'MAUT1', 'MAUT0', 'MZFONDS',\n","       'MZPART', 'MINKM30', 'MINK3045', 'MINK4575', 'MINK7512', 'MINKGEM',\n","       'MKOOPKLA', 'PWAPART', 'PPERSAUT', 'PBRAND', 'AWAPART', 'APERSAUT',\n","       'ALEVEN', 'ABRAND']]\n","Y = df_train_y\n"," \n","classifier = LogisticRegression(random_state = 0) \n","classifier.fit(X_fselect, Y)\n","\n","y_pred = classifier.predict(X_testfselect)\n","\n","\n","\n","cm = confusion_matrix(df_test_y,y_pred) \n","  \n","print (\"Confusion Matrix : \\n\", cm) \n","\n","  \n","print (\"Accuracy : \", accuracy_score(df_test_y,y_pred))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Confusion Matrix : \n"," [[3762    0]\n"," [ 238    0]]\n","Accuracy :  0.9405\n"],"name":"stdout"},{"output_type":"stream","text":["C:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n","  FutureWarning)\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"xXak1EBPeZqZ","outputId":"c33c2a8d-e933-4596-94b9-9b836ee837fe"},"source":["TP = cm[0, 0]\n","TN = cm[1, 1]\n","FP = cm[0, 1]\n","FN = cm[1, 0]\n","\n","print((TP + TN) / float(TP + TN + FP + FN))\n","print(metrics.accuracy_score(df_test_y, y_pred))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["0.9405\n","0.9405\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"lsp-sl99eZqZ","outputId":"73317247-690d-4845-aeff-74288dba3675"},"source":["#recall\n","print(TP / float(TP + FN))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["0.9405\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"0cu2OWMreZqa"},"source":["Using feature importance to chose which variables to use in our random forest gives us an accuracy of 0.94 just like the first two tests. Feature importance feature selection recommended 40 different variables to describe the dataset. We also see in the confusion matrix that false positives and true positives both went to 0. "]},{"cell_type":"markdown","metadata":{"id":"pVb0YUHVeZqa"},"source":["# 4. Conclusion"]},{"cell_type":"markdown","metadata":{"id":"WeMuFs24eZqa"},"source":["We tried to select features in the dataset three different ways: stepwise selection based on p-values, Principal component analysis, and feature importance. \n","\n","The first test run, Logistic Regression with all features, was used as a base to see the accuracy of all the features in determining whether or not someone would be interested in buying caravan insurance policy or not. This first test had an accuracy of .94 using 85 features to determine the categorization. \n","\n","The second test run, Logistic Regression with stepwise selection based on p-values, determined that only 16 features were neccessary to determine the categorization of the data. The accuracy of the logistic regression of the 16 feature dataset was 0.94. This shows, compared to our base case, that we can achieve the same accuracy in categorization with only 16 features instead of 85 features.\n","\n","The third test run used Random Forest based on Principal Component Analysis feature selection. The PCA suggests to show %90 of the variance we only need 13 principal components. That is 13 eigenvalues can describe %90 of the variance. I was unable to show the accuracy of this within a Random Forest.\n","\n","The fourth and final test used Random Forest based on feature importance feature selection. Feature importance feature selection recommended 40 different variables to describe the dataset. Using the 40 variables recommended our accuracy from the Random Forest was 0.94 just like the first two tests. "]},{"cell_type":"markdown","metadata":{"id":"1bdQnMt1eZqa"},"source":["### a.) Problems and Thoughts"]},{"cell_type":"markdown","metadata":{"id":"5efZxHkCeZqa"},"source":["I am very confused as to how to apply the PCA to the random forest classifier. When performing a PCA it will output the principal components. The principal components are normalized linear combinations of the features in the original dataset what would be the dependent value? I tried to run the principal component score (variance_ratio_) as y and principal component as x but would get errors. My thought was to run this on the training set to then predict what would happen on the test set.\n","\n","I need to look into classifying my models in better ways. I was not sure whether to use precision, accuracy, recall, etc. on this problem. I read in an article about why accuracy isn't the best to go off of because it doesn't give a good picture. It is also not the best when the class is not balanced which it wasn't in this case. "]}]}
