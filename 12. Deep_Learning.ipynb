{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.4"},"colab":{"name":"Deep_Learning.ipynb","provenance":[],"collapsed_sections":[]}},"cells":[{"cell_type":"markdown","metadata":{"id":"EdiCfcchfJTE"},"source":["# InClass 12\n","## Quentin Smith\n","## 4/16/20"]},{"cell_type":"markdown","metadata":{"id":"bP0I3LslfJTL"},"source":["## My Attempt at creating my own CNN\n","\n","Using: https://heartbeat.fritz.ai/basics-of-image-classification-with-pytorch-2f8973c51864\n","\n","Options: https://pytorch.org/docs/stable/nn.html#"]},{"cell_type":"markdown","metadata":{"id":"BnHe18IKfJTM"},"source":["Exercise im changin: https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html"]},{"cell_type":"code","metadata":{"id":"AMGHedFTfJTM"},"source":["import torch\n","import torchvision\n","import torchvision.transforms as transforms\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ze40yZdRfJTN","outputId":"e40cf132-2908-45f7-f09f-b90ddc7342ec"},"source":["transform = transforms.Compose(\n","    [transforms.ToTensor(),\n","     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n","\n","\n","#Define transformations for the training set, flip the images randomly, crop out and apply mean and std normalization\n","# train_transformations = transforms.Compose([\n","#     transforms.RandomHorizontalFlip(),\n","#     transforms.RandomCrop(32,padding=4),\n","#     transforms.ToTensor(),\n","#     transforms.Normalize((0.5,0.5,0.5), (0.5,0.5,0.5))\n","# ])\n","\n","\n","# test_transformations = transforms.Compose([\n","#     transforms.ToTensor(),\n","#     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n","\n","# ])\n","\n","\n","trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n","                                        download=True, transform=transform)\n","trainloader = torch.utils.data.DataLoader(trainset, batch_size=4,\n","                                          shuffle=True, num_workers=2)\n","\n","testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n","                                       download=True, transform=transform)\n","testloader = torch.utils.data.DataLoader(testset, batch_size=4,\n","                                         shuffle=False, num_workers=2)\n","\n","classes = ('plane', 'car', 'bird', 'cat',\n","           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Files already downloaded and verified\n","Files already downloaded and verified\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"qNK6dMO4fJTO","outputId":"5fec4fc0-ef85-4d87-fed9-5186ee7bb17f"},"source":["import matplotlib.pyplot as plt\n","import numpy as np\n","\n","# functions to show an image\n","\n","\n","def imshow(img):\n","    img = img / 2 + 0.5     # unnormalize\n","    npimg = img.numpy()\n","    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n","    plt.show()\n","\n","\n","# get some random training images\n","dataiter = iter(trainloader)\n","images, labels = dataiter.next()\n","\n","# show images\n","imshow(torchvision.utils.make_grid(images))\n","# print labels\n","print(' '.join('%5s' % classes[labels[j]] for j in range(4)))"],"execution_count":null,"outputs":[{"output_type":"stream","text":[" bird  frog   car truck\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"MaR6DvtifJTO"},"source":["## Define a Convolutional Neural Network"]},{"cell_type":"code","metadata":{"id":"XLH-xLV2fJTO"},"source":["import torch.nn as nn\n","import torch.nn.functional as F\n","\n","# Unit taken from example: https://heartbeat.fritz.ai/basics-of-image-classification-with-pytorch-2f8973c51864\n","class Unit(nn.Module):\n","    def __init__(self,in_channels,out_channels):\n","        super(Unit,self).__init__()\n","\n","        self.conv = nn.Conv2d(in_channels=in_channels,kernel_size=3,out_channels=out_channels,stride=1,padding=1)\n","        self.bn = nn.BatchNorm2d(num_features=out_channels)\n","        self.relu = nn.ReLU()\n","\n","    def forward(self,input):\n","        output = self.conv(input)\n","        output = self.bn(output)\n","        output = self.relu(output)\n","\n","        return output\n","\n","class Net(nn.Module):\n","    def __init__(self,num_classes=10):\n","        super(Net,self).__init__()\n","\n","        #Create 14 layers of the unit with max pooling in between\n","        self.unit1 = Unit(in_channels=3,out_channels=32)\n","        self.unit2 = Unit(in_channels=32, out_channels=32)\n","        self.unit3 = Unit(in_channels=32, out_channels=32)\n","\n","        self.pool1 = nn.MaxPool2d(kernel_size=2)\n","\n","        self.unit4 = Unit(in_channels=32, out_channels=64)\n","        self.unit5 = Unit(in_channels=64, out_channels=64)\n","        self.unit6 = Unit(in_channels=64, out_channels=64)\n","        self.unit7 = Unit(in_channels=64, out_channels=64)\n","\n","        self.pool2 = nn.MaxPool2d(kernel_size=2)\n","\n","        self.unit8 = Unit(in_channels=64, out_channels=128)\n","        self.unit9 = Unit(in_channels=128, out_channels=128)\n","        self.unit10 = Unit(in_channels=128, out_channels=128)\n","        self.unit11 = Unit(in_channels=128, out_channels=128)\n","\n","        self.pool3 = nn.MaxPool2d(kernel_size=2)\n","\n","        self.unit12 = Unit(in_channels=128, out_channels=256)\n","        self.unit13 = Unit(in_channels=256, out_channels=256)\n","        self.unit14 = Unit(in_channels=256, out_channels=256)\n","        self.unit15 = Unit(in_channels=256, out_channels=256)\n","        \n","        self.avgpool = nn.AvgPool2d(kernel_size=4)\n","\n","        #Add all the units into the Sequential layer in exact order\n","        self.net = nn.Sequential(self.unit1, self.unit2, self.unit3, self.pool1, self.unit4, self.unit5, self.unit6\n","                                ,self.unit7, self.pool2, self.unit8, self.unit9, self.unit10, self.unit11, self.pool3,\n","                                self.unit12, self.unit13, self.unit14, self.unit15, self.avgpool)\n","\n","        self.fc = nn.Linear(in_features=256,out_features=num_classes)\n","\n","    def forward(self, input):\n","        output = self.net(input)\n","        output = output.view(-1,256)\n","        output = self.fc(output)\n","        return output\n","\n","net=Net()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"cS3X-8MefJTP"},"source":["## Define a Loss Function and Optimizer"]},{"cell_type":"code","metadata":{"id":"2sDt6J32fJTP"},"source":["import torch.optim as optim\n","\n","criterion = nn.CrossEntropyLoss()\n","optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"tnGbhoL0fJTP"},"source":["## Train the Network"]},{"cell_type":"code","metadata":{"id":"9AFymA_6fJTP","outputId":"e52dd982-539c-44c7-a938-e9c8ba16efc1"},"source":["for epoch in range(2):  # loop over the dataset multiple times\n","\n","    running_loss = 0.0\n","    for i, data in enumerate(trainloader, 0):\n","        # get the inputs; data is a list of [inputs, labels]\n","        inputs, labels = data\n","\n","        # zero the parameter gradients\n","        optimizer.zero_grad()\n","\n","        # forward + backward + optimize\n","        outputs = net(inputs)\n","        loss = criterion(outputs, labels)\n","        loss.backward()\n","        optimizer.step()\n","\n","        # print statistics\n","        running_loss += loss.item()\n","        if i % 2000 == 1999:    # print every 2000 mini-batches\n","            print('[%d, %5d] loss: %.3f' %\n","                  (epoch + 1, i + 1, running_loss / 2000))\n","            running_loss = 0.0\n","\n","print('Finished Training')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[1,  2000] loss: 1.951\n","[1,  4000] loss: 1.721\n","[1,  6000] loss: 1.527\n","[1,  8000] loss: 1.383\n","[1, 10000] loss: 1.272\n","[1, 12000] loss: 1.181\n","[2,  2000] loss: 1.074\n","[2,  4000] loss: 1.036\n","[2,  6000] loss: 0.990\n","[2,  8000] loss: 0.966\n","[2, 10000] loss: 0.920\n","[2, 12000] loss: 0.882\n","Finished Training\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"pPNpeFyZfJTQ"},"source":["PATH = './cifar_net.pth'\n","torch.save(net.state_dict(), PATH)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"cN-2WDQsfJTQ"},"source":["## Test the Network o n the Test Data"]},{"cell_type":"code","metadata":{"id":"GiWZe-WyfJTR","outputId":"4f9b3700-43c7-4e33-bbf0-0fbf50a0daac"},"source":["dataiter = iter(testloader)\n","images, labels = dataiter.next()\n","\n","# print images\n","imshow(torchvision.utils.make_grid(images))\n","print('GroundTruth: ', ' '.join('%5s' % classes[labels[j]] for j in range(4)))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["GroundTruth:    cat  ship  ship plane\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"t0R2ZtW4fJTR","outputId":"e806ae79-8735-4219-a3c1-8c4f664a11a5"},"source":["net.load_state_dict(torch.load(PATH))"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<All keys matched successfully>"]},"metadata":{"tags":[]},"execution_count":131}]},{"cell_type":"code","metadata":{"id":"eMn8l81ofJTR"},"source":["outputs = net(images)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"0dwDXdv7fJTS","outputId":"808dba22-2f2d-4db7-b569-53e4d57d8ca1"},"source":["_, predicted = torch.max(outputs, 1)\n","\n","print('Predicted: ', ' '.join('%5s' % classes[predicted[j]]\n","                              for j in range(4)))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Predicted:    cat  ship  ship plane\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"6E1A8zyOfJTS","outputId":"dd8ff4d9-cef6-4e87-95ea-1c4696686b19"},"source":["correct = 0\n","total = 0\n","with torch.no_grad():\n","    for data in testloader:\n","        images, labels = data\n","        outputs = net(images)\n","        _, predicted = torch.max(outputs.data, 1)\n","        total += labels.size(0)\n","        correct += (predicted == labels).sum().item()\n","\n","print('Accuracy of the network on the 10000 test images: %d %%' % (\n","    100 * correct / total))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Accuracy of the network on the 10000 test images: 70 %\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"UxZ52y5JfJTS","outputId":"8990adb0-efd8-44d1-a348-37708e441cd8"},"source":["class_correct = list(0. for i in range(10))\n","class_total = list(0. for i in range(10))\n","with torch.no_grad():\n","    for data in testloader:\n","        images, labels = data\n","        outputs = net(images)\n","        _, predicted = torch.max(outputs, 1)\n","        c = (predicted == labels).squeeze()\n","        for i in range(4):\n","            label = labels[i]\n","            class_correct[label] += c[i].item()\n","            class_total[label] += 1\n","\n","\n","for i in range(10):\n","    print('Accuracy of %5s : %2d %%' % (\n","        classes[i], 100 * class_correct[i] / class_total[i]))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Accuracy of plane : 71 %\n","Accuracy of   car : 86 %\n","Accuracy of  bird : 61 %\n","Accuracy of   cat : 50 %\n","Accuracy of  deer : 64 %\n","Accuracy of   dog : 58 %\n","Accuracy of  frog : 74 %\n","Accuracy of horse : 81 %\n","Accuracy of  ship : 87 %\n","Accuracy of truck : 71 %\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"fAJa-NdUfJTS"},"source":["# Results:"]},{"cell_type":"markdown","metadata":{"id":"X-ALiDhnfJTT"},"source":["Original from website:\n","\n","Accuracy of the network on the 10000 test images: 54 %\n","\n","First Attempt:\n","Switching the CNN to the https://heartbeat.fritz.ai/basics-of-image-classification-with-pytorch-2f8973c51864\n","Accuracy of the network on the 10000 test images: 55 %"]},{"cell_type":"markdown","metadata":{"id":"ytYHcLzwfJTT"},"source":["Second Attempt: Running same test as attempt one:\n","Accuracy of the network on the 10000 test images: 69 %"]},{"cell_type":"markdown","metadata":{"id":"vcsPnyqpfJTT"},"source":["Third Attempt: Changed epoch to 4. Took much longer. Accuracy of the network on the 10000 test images: 76 %\n"]},{"cell_type":"markdown","metadata":{"id":"wzEFRzAifJTT"},"source":["Fourth Attemp: Changed epoch to 1. Accuracy of the network on the 10000 test images: 78 %"]},{"cell_type":"markdown","metadata":{"id":"2Y_W7KatfJTT"},"source":["Accuracy of the network on the 10000 test images: 70 % Changing to 256"]},{"cell_type":"code","metadata":{"id":"A2h85aHAfJTT"},"source":[""],"execution_count":null,"outputs":[]}]}
