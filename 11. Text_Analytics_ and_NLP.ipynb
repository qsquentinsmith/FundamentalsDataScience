{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.4"},"colab":{"name":"Text_Analytics_NLP.ipynb","provenance":[],"collapsed_sections":[]}},"cells":[{"cell_type":"markdown","metadata":{"id":"JPAP5DfHe-y3"},"source":["# In Class 11\n","## Author: Quentin Smith\n","## INET 4061\n","\n","## Date Due: 4/12/20\n"]},{"cell_type":"markdown","metadata":{"id":"qe0ySzbme-y5"},"source":["### Introduction:"]},{"cell_type":"markdown","metadata":{"id":"GcWgkZr_e-y5"},"source":["Complete steps 1-7 from https://www.digitalocean.com/community/tutorials/how-to-perform-sentiment-analysis-in-python-3-using-the-natural-language-toolkit-nltk\n","\n","Then answer questions based on steps 3, 4, 5, and 7. "]},{"cell_type":"markdown","metadata":{"id":"R5Sz6rfte-y6"},"source":["## Step 1: Installing NLTK and Downloading the Data"]},{"cell_type":"code","metadata":{"id":"pR5JeKcye-y6","outputId":"ed6b23af-dfd6-4aab-dbff-6feef9b8a248"},"source":["#------Install nltk if not already downloaded-----\n","#pip install nltk==3.3\n","\n","import nltk\n","\n","#------get data and store locally-----\n","nltk.download('twitter_samples')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading package twitter_samples to\n","[nltk_data]     C:\\Users\\hv2486co\\AppData\\Roaming\\nltk_data...\n","[nltk_data]   Package twitter_samples is already up-to-date!\n"],"name":"stderr"},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{"tags":[]},"execution_count":1}]},{"cell_type":"markdown","metadata":{"id":"Mf8yTK6Te-y7"},"source":["## Step 2: Tokenizing the Data"]},{"cell_type":"code","metadata":{"id":"PyViXZw2e-y8","outputId":"fcde3c47-5e8a-4c73-ac46-cd01758eb066"},"source":["#------- punkt is a pre-trained model that helps tokenize words and sentences ------\n","nltk.download('punkt')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading package punkt to\n","[nltk_data]     C:\\Users\\hv2486co\\AppData\\Roaming\\nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n"],"name":"stderr"},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{"tags":[]},"execution_count":2}]},{"cell_type":"code","metadata":{"id":"PUcDcJqXe-y8","outputId":"96211f3c-3486-4a28-9f63-061da0761111"},"source":["from nltk.corpus import twitter_samples\n","\n","#------Create variables for positive, negative, and text-------\n","positive_tweets = twitter_samples.strings('positive_tweets.json')\n","negative_tweets = twitter_samples.strings('negative_tweets.json')\n","text = twitter_samples.strings('tweets.20150430-223406.json')\n","\n","#--------tokenize positive_tweets creates array of each tweet tokenized--------\n","tweet_tokens = twitter_samples.tokenized('positive_tweets.json')\n","\n","print(tweet_tokens[0])"],"execution_count":null,"outputs":[{"output_type":"stream","text":["['#FollowFriday', '@France_Inte', '@PKuchly57', '@Milipol_Paris', 'for', 'being', 'top', 'engaged', 'members', 'in', 'my', 'community', 'this', 'week', ':)']\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"gfTOFzN6e-y8"},"source":["## Step 3. Normalize the Data"]},{"cell_type":"code","metadata":{"id":"GbbSIY0je-y8","outputId":"2347a228-21fe-4815-b95c-e2edc6b9fbe4"},"source":["# wordnet is a lexical database for the english language that helps the script determine the base word\n","nltk.download('wordnet')\n","\n","# resource to determine the context of a word in a sentence\n","nltk.download('averaged_perceptron_tagger')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading package wordnet to\n","[nltk_data]     C:\\Users\\hv2486co\\AppData\\Roaming\\nltk_data...\n","[nltk_data]   Package wordnet is already up-to-date!\n","[nltk_data] Downloading package averaged_perceptron_tagger to\n","[nltk_data]     C:\\Users\\hv2486co\\AppData\\Roaming\\nltk_data...\n","[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n","[nltk_data]       date!\n"],"name":"stderr"},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{"tags":[]},"execution_count":4}]},{"cell_type":"code","metadata":{"id":"Ozf2im1ae-y9","outputId":"039d5539-e374-4b5d-e0d2-6ff586b31416"},"source":["#full list of tags https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html\n","from nltk.tag import pos_tag\n","from nltk.corpus import twitter_samples\n","\n","#tokenizes and tags tweets\n","tweet_tokens = twitter_samples.tokenized('positive_tweets.json')\n","print(pos_tag(tweet_tokens[1]))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[('@Lamb2ja', 'NN'), ('Hey', 'NNP'), ('James', 'NNP'), ('!', '.'), ('How', 'NNP'), ('odd', 'JJ'), (':/', 'NNP'), ('Please', 'NNP'), ('call', 'VB'), ('our', 'PRP$'), ('Contact', 'NNP'), ('Centre', 'NNP'), ('on', 'IN'), ('02392441234', 'CD'), ('and', 'CC'), ('we', 'PRP'), ('will', 'MD'), ('be', 'VB'), ('able', 'JJ'), ('to', 'TO'), ('assist', 'VB'), ('you', 'PRP'), (':)', 'VBP'), ('Many', 'JJ'), ('thanks', 'NNS'), ('!', '.')]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"nASgsdCLe-y9"},"source":["from nltk.tag import pos_tag\n","from nltk.stem.wordnet import WordNetLemmatizer\n","\n","#The function lemmatize_sentence first gets the position tag of each token of a tweet and assigns the value of the tag\n","def lemmatize_sentence(tokens):\n","    lemmatizer = WordNetLemmatizer()\n","    lemmatized_sentence = []\n","    for word, tag in pos_tag(tokens):\n","        if tag.startswith('NN'):\n","            pos = 'n'\n","        elif tag.startswith('VB'):\n","            pos = 'v'\n","        else:\n","            pos = 'a'\n","        lemmatized_sentence.append(lemmatizer.lemmatize(word, pos))\n","    return lemmatized_sentence"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"nGfITdw9e-y9"},"source":["## Step 4: Remove Noise from the Data"]},{"cell_type":"code","metadata":{"id":"rAWScZVZe-y9"},"source":["#Noise includes hyperlinks, twitter handles in replies, and punctuation and special characters\n","\n","import re, string\n","\n","def remove_noise(tweet_tokens, stop_words = ()):\n","\n","    cleaned_tokens = []\n","\n","    for token, tag in pos_tag(tweet_tokens):\n","        token = re.sub('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+#]|[!*\\(\\),]|'\\\n","                       '(?:%[0-9a-fA-F][0-9a-fA-F]))+','', token)\n","        token = re.sub(\"(@[A-Za-z0-9_]+)\",\"\", token)\n","\n","        if tag.startswith(\"NN\"):\n","            pos = 'n'\n","        elif tag.startswith('VB'):\n","            pos = 'v'\n","        else:\n","            pos = 'a'\n","\n","        lemmatizer = WordNetLemmatizer()\n","        token = lemmatizer.lemmatize(token, pos)\n","\n","        if len(token) > 0 and token not in string.punctuation and token.lower() not in stop_words:\n","            cleaned_tokens.append(token.lower())\n","    return cleaned_tokens"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"lSePrXZ2e-y-","outputId":"afeb8589-9a99-4b98-cc9c-5864f82d020d"},"source":["nltk.download('stopwords')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading package stopwords to\n","[nltk_data]     C:\\Users\\hv2486co\\AppData\\Roaming\\nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n"],"name":"stderr"},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{"tags":[]},"execution_count":8}]},{"cell_type":"code","metadata":{"id":"pfI9l6dNe-y-"},"source":["from nltk.corpus import stopwords\n","stop_words = stopwords.words('english')\n","\n","#print(remove_noise(tweet_tokens[0], stop_words))\n","\n","positive_tweet_tokens = twitter_samples.tokenized('positive_tweets.json')\n","negative_tweet_tokens = twitter_samples.tokenized('negative_tweets.json')\n","\n","#clean both pos and neg tweet list\n","positive_cleaned_tokens_list = []\n","negative_cleaned_tokens_list = []\n","\n","\n","for tokens in positive_tweet_tokens:\n","    positive_cleaned_tokens_list.append(remove_noise(tokens, stop_words))\n","\n","for tokens in negative_tweet_tokens:\n","    negative_cleaned_tokens_list.append(remove_noise(tokens, stop_words))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"M7w_AU58e-y-"},"source":["#---- compare the output versions of the 500th tweet in the list----\n","\n","#print(positive_tweet_tokens[500])\n","#print(positive_cleaned_tokens_list[500])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"8f5u4CFme-y-"},"source":["## Step 5. Determining Word Density"]},{"cell_type":"code","metadata":{"id":"HD3idwabe-y-"},"source":["#---- takes a list of tweets and provides a list of words in all of the tweets joined\n","def get_all_words(cleaned_tokens_list):\n","    for tokens in cleaned_tokens_list:\n","        for token in tokens:\n","            yield token\n","\n","all_pos_words = get_all_words(positive_cleaned_tokens_list)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"MOJMdNCVe-y_","outputId":"68ff0ba3-3a60-4683-b9a4-57a9ad71b5dc"},"source":["from nltk import FreqDist\n","\n","#find frequency of top ten words\n","freq_dist_pos = FreqDist(all_pos_words)\n","print(freq_dist_pos.most_common(10))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[(':)', 3691), (':-)', 701), (':d', 658), ('thanks', 388), ('follow', 357), ('love', 333), ('...', 290), ('good', 283), ('get', 263), ('thank', 253)]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"_9li6mz8e-y_"},"source":["## Step 6. Preparing Data for the Model"]},{"cell_type":"code","metadata":{"id":"FPXPo-B3e-y_"},"source":["#---- convert tweets from a list of cleaned tokens to dictionaries with keys as the tokens and Truw as values\n","def get_tweets_for_model(cleaned_tokens_list):\n","    for tweet_tokens in cleaned_tokens_list:\n","        yield dict([token, True] for token in tweet_tokens)\n","\n","positive_tokens_for_model = get_tweets_for_model(positive_cleaned_tokens_list)\n","negative_tokens_for_model = get_tweets_for_model(negative_cleaned_tokens_list)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"MyBtlpqRe-y_"},"source":["import random\n","\n","\n","#attach posisitve or negative label to each tweet. \n","\n","positive_dataset = [(tweet_dict, \"Positive\")\n","                     for tweet_dict in positive_tokens_for_model]\n","\n","negative_dataset = [(tweet_dict, \"Negative\")\n","                     for tweet_dict in negative_tokens_for_model]\n","\n","#creates dataset by joining the positive and negative datasets\n","dataset = positive_dataset + negative_dataset\n","\n","\n","random.shuffle(dataset)\n","\n","train_data = dataset[:7000]\n","test_data = dataset[7000:]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"g4DXYD97e-y_"},"source":["## Step 7. Building and Testing the Model"]},{"cell_type":"code","metadata":{"id":"4pNLGu9te-y_","outputId":"749c99b3-7a61-42cf-d8d6-ccccdbc89a53"},"source":["from nltk import classify\n","from nltk import NaiveBayesClassifier\n","\n","classifier = NaiveBayesClassifier.train(train_data)\n","\n","print(\"Accuracy is:\", classify.accuracy(classifier, test_data))\n","\n","print(classifier.show_most_informative_features(10))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Accuracy is: 0.9946666666666667\n","Most Informative Features\n","                      :( = True           Negati : Positi =   2079.7 : 1.0\n","                      :) = True           Positi : Negati =    981.3 : 1.0\n","                     sad = True           Negati : Positi =     55.4 : 1.0\n","                follower = True           Positi : Negati =     20.4 : 1.0\n","                    glad = True           Positi : Negati =     19.5 : 1.0\n","                     x15 = True           Negati : Positi =     19.1 : 1.0\n","                    blog = True           Positi : Negati =     17.5 : 1.0\n","               community = True           Positi : Negati =     17.5 : 1.0\n","                followed = True           Negati : Positi =     13.5 : 1.0\n","                 perfect = True           Positi : Negati =     12.9 : 1.0\n","None\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Q3YV2n0Xe-zA","outputId":"38a1e216-e7fa-42e8-8487-3731472378b1"},"source":["from nltk.tokenize import word_tokenize\n","\n","# test custom tweets\n","custom_tweet = \"I ordered just once from TerribleCo, they screwed up, never used the app again.\"\n","\n","custom_tokens = remove_noise(word_tokenize(custom_tweet))\n","\n","print(classifier.classify(dict([token, True] for token in custom_tokens)))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Positive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"nQGaesmFe-zA","outputId":"220811eb-4013-4b89-bf09-3121067c1d56"},"source":["custom_tweet = 'Congrats #SportStar on your 7th best goal from last season winning goal of the year :) #Baller #Topbin #oneofmanyworldies'\n","\n","custom_tokens = remove_noise(word_tokenize(custom_tweet))\n","\n","print(classifier.classify(dict([token, True] for token in custom_tokens)))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Positive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"nNseWnRHe-zA","outputId":"59b3beab-7aaa-471a-8381-b4d19247912f"},"source":["custom_tweet = 'Thank you for sending my baggage to CityX and flying me to CityY at the same time. Brilliant service. #thanksGenericAirline'\n","\n","custom_tokens = remove_noise(word_tokenize(custom_tweet))\n","\n","print(classifier.classify(dict([token, True] for token in custom_tokens)))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Positive\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"XtksDhMOe-zB"},"source":["# Questions 1:"]},{"cell_type":"markdown","metadata":{"id":"AkBak9Nde-zB"},"source":["From Step 3:\n","    1. List 10 Proper nouns that exist in the negative data set. "]},{"cell_type":"code","metadata":{"scrolled":true,"id":"CQ6rT-LYe-zB","outputId":"9d775d8f-2e2f-46cd-c2c2-5d2586bad448"},"source":["#full list of tags https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html\n","from nltk.tag import pos_tag\n","from nltk.corpus import twitter_samples\n","from nltk.stem.wordnet import WordNetLemmatizer\n","\n","#tokenizes and tags tweets\n","tweet_tokens = twitter_samples.tokenized('positive_tweets.json')\n","#print(pos_tag(tweet_tokens[1]))\n","\n","\n","\n","#The function lemmatize_sentence first gets the position tag of each token of a tweet and assigns the value of the tag\n","def lemmatize_sentence(tokens):\n","    lemmatizer = WordNetLemmatizer()\n","    lemmatized_sentence = []\n","    for word, tag in pos_tag(tokens):\n","        if tag.startswith('NNP'):\n","            print('Proper Noun', word)\n","\n","print(pos_tag(tweet_tokens[0]))\n","print(lemmatize_sentence(tweet_tokens[0]))\n","\n","print(pos_tag(tweet_tokens[1]))\n","print(lemmatize_sentence(tweet_tokens[1]))\n","    "],"execution_count":null,"outputs":[{"output_type":"stream","text":["[('#FollowFriday', 'JJ'), ('@France_Inte', 'NNP'), ('@PKuchly57', 'NNP'), ('@Milipol_Paris', 'NNP'), ('for', 'IN'), ('being', 'VBG'), ('top', 'JJ'), ('engaged', 'VBN'), ('members', 'NNS'), ('in', 'IN'), ('my', 'PRP$'), ('community', 'NN'), ('this', 'DT'), ('week', 'NN'), (':)', 'NN')]\n","Proper Noun @France_Inte\n","Proper Noun @PKuchly57\n","Proper Noun @Milipol_Paris\n","None\n","[('@Lamb2ja', 'NN'), ('Hey', 'NNP'), ('James', 'NNP'), ('!', '.'), ('How', 'NNP'), ('odd', 'JJ'), (':/', 'NNP'), ('Please', 'NNP'), ('call', 'VB'), ('our', 'PRP$'), ('Contact', 'NNP'), ('Centre', 'NNP'), ('on', 'IN'), ('02392441234', 'CD'), ('and', 'CC'), ('we', 'PRP'), ('will', 'MD'), ('be', 'VB'), ('able', 'JJ'), ('to', 'TO'), ('assist', 'VB'), ('you', 'PRP'), (':)', 'VBP'), ('Many', 'JJ'), ('thanks', 'NNS'), ('!', '.')]\n","Proper Noun Hey\n","Proper Noun James\n","Proper Noun How\n","Proper Noun :/\n","Proper Noun Please\n","Proper Noun Contact\n","Proper Noun Centre\n","None\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"_1VqBo26e-zB"},"source":["## Ten Proper Nouns\n","1. @France_Inte\n","2. @PKuchly57\n","3. @Milipol_Paris\n","4. Hey\n","5. James\n","6. How\n","7. :/\n","8. Please\n","9. Contact\n","10. Centre"]},{"cell_type":"markdown","metadata":{"id":"O1Tq8DSOe-zC"},"source":["# Question 2"]},{"cell_type":"markdown","metadata":{"id":"lSRwfq_Me-zC"},"source":["Step 4\n","2. Pick a random tweet (tweet_tokens[PICK A NUMBER]) and explain the difference you see in the output between the lemmatize_sentence function and the remove_noise function\n"]},{"cell_type":"code","metadata":{"id":"Pllb0OV9e-zC","outputId":"929ceb70-b0bb-489b-f510-910958fe5238"},"source":["#---- compare the output versions of the 500th tweet in the list----\n","print('Original Output: ', tweet_tokens[0])\n","\n","print('\\nLemmatized output', lemmatize_sentence(tweet_tokens[0]))\n","\n","print('\\nRemove Noise Output ',remove_noise(tweet_tokens[0], stop_words))\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Original Output:  ['#FollowFriday', '@France_Inte', '@PKuchly57', '@Milipol_Paris', 'for', 'being', 'top', 'engaged', 'members', 'in', 'my', 'community', 'this', 'week', ':)']\n","Proper Noun @France_Inte\n","Proper Noun @PKuchly57\n","Proper Noun @Milipol_Paris\n","\n","Lemmatized output None\n","\n","Remove Noise Output  ['#followfriday', 'top', 'engage', 'member', 'community', 'week', ':)']\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"kBQ64waNe-zC"},"source":["\n","Lemmatization normalizes the word. In our case it removes the affixes. Some examples from the original output are being --> be, engaged --> engage, members --> member. \n","\n","The remove noise function gets rid of hyperlinks, twitter handles, and punctuation and special characters and replaces them with an empty string. \n","\n","So from Lemmatization to Remove_Noise we see that the twitter handles are changed to empty strings (@France_Inte, @PKuchly57, @Milipol_Paris), removes stop words (for, be, in, my this), and removes punctuation and symbols (:). "]},{"cell_type":"markdown","metadata":{"id":"FDKNLIlQe-zD"},"source":["# Question 3"]},{"cell_type":"markdown","metadata":{"id":"EDWggPeVe-zD"},"source":["Step 5\n","3. What are the top 10 most negative words by frequency?"]},{"cell_type":"code","metadata":{"id":"crav1cyUe-zD","outputId":"175fa7be-def7-4027-bfc2-69a61d0d1438"},"source":["def get_all_words(cleaned_tokens_list):\n","    for tokens in cleaned_tokens_list:\n","        for token in tokens:\n","            yield token\n","\n","all_neg_words = get_all_words(negative_cleaned_tokens_list)\n","\n","#find frequency of top ten words\n","freq_dist_neg = FreqDist(all_neg_words)\n","print(freq_dist_neg.most_common(10))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[(':(', 4585), (':-(', 501), (\"i'm\", 343), ('...', 332), ('get', 325), ('miss', 291), ('go', 275), ('please', 275), ('want', 246), ('like', 218)]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"grSba1e7e-zD"},"source":["# Question 4"]},{"cell_type":"markdown","metadata":{"id":"ixNJx-oce-zD"},"source":["Step 7\n","4. Is “community” a positive or negative word under informative features?"]},{"cell_type":"code","metadata":{"id":"n_wo_pBae-zD","outputId":"1fdd0c7a-e0d3-4b0d-950e-2c74380fe5d8"},"source":["print(classifier.show_most_informative_features(10))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Most Informative Features\n","                      :( = True           Negati : Positi =   2079.7 : 1.0\n","                      :) = True           Positi : Negati =    981.3 : 1.0\n","                     sad = True           Negati : Positi =     55.4 : 1.0\n","                follower = True           Positi : Negati =     20.4 : 1.0\n","                    glad = True           Positi : Negati =     19.5 : 1.0\n","                     x15 = True           Negati : Positi =     19.1 : 1.0\n","                    blog = True           Positi : Negati =     17.5 : 1.0\n","               community = True           Positi : Negati =     17.5 : 1.0\n","                followed = True           Negati : Positi =     13.5 : 1.0\n","                 perfect = True           Positi : Negati =     12.9 : 1.0\n","None\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"Mppwamjle-zD"},"source":["Community is associated with positive tweets 17.1: 1.0."]},{"cell_type":"markdown","metadata":{"id":"FmLb8afke-zE"},"source":["# Question 5\n","\n","5. Go to Twitter (assuming you have it, otherwise you can google it).  Find a tweet from a famous person of your choice and copy it into the custom_tweet object and run your classifier.  Was it positive or negative?"]},{"cell_type":"code","metadata":{"id":"h1_uHcIGe-zE","outputId":"90f8e785-7015-4aca-b0ea-218cb52f4a70"},"source":["# From Jaden Smith\n","\n","\n","# test custom tweets\n","custom_tweet = \"How Can Mirrors Be Real If Our Eyes Aren't Real\"\n","\n","custom_tokens = remove_noise(word_tokenize(custom_tweet))\n","\n","print(classifier.classify(dict([token, True] for token in custom_tokens)))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Negative\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"SiR_SSa6e-zE"},"source":["# Question 6\n","6. What did you learn from this exercise?"]},{"cell_type":"markdown","metadata":{"id":"_OUM0c7Ke-zE"},"source":["Here is a list of things that I learned:\n","1. There is a lot of data preprocessing that goes along with text analysis. \n","2. There are very logical steps and dictionaries that you cross reference words with. \n","3. Just because the classifier says a tweet is negative doesn't mean it is actually a negative sentiment. Case in point the famous tweet I used, in my mind, does not have a negative sentiment. It is more neutral when I read it. \n","4. Sentiment analysis is still subjective and could be improved."]},{"cell_type":"markdown","metadata":{"id":"YoaYBouHe-zE"},"source":["# Bonus:"]},{"cell_type":"code","metadata":{"id":"VUpeaO6Ze-zF","outputId":"7437c6c6-b90e-4869-c290-c1b15f009265"},"source":["from nltk import classify\n","from nltk import DecisionTreeClassifier\n","\n","classif = DecisionTreeClassifier.train(train_data)\n","\n","print(\"Accuracy is:\", classify.accuracy(classif, test_data))\n","\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Accuracy is: 0.995\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"Af_LWPALe-zF"},"source":["Decision Tree Classifier Accuracy is: 0.995\n","Naive Bayes Classifier Accuracy is: 0.9946666666666667\n","\n","It looks like they both have the same accuracy. I hopefully have run the Decision Tree Classifier correctly. What did differe is the time it took to run both of the classifiers. Naive Bayes is less than a minute or so. Decision Tree takes around 20 minutes. "]},{"cell_type":"code","metadata":{"id":"vUWlhT3Je-zF"},"source":[""],"execution_count":null,"outputs":[]}]}
